# -*- coding: utf-8 -*-
"""ComparingBasicNNArchitectures.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qnRXqeGkYiuJ7b3ruNx81Bu84cK7OPli
"""

#import core module
from tensorflow import keras

#other useful materials
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras import utils
from numpy import mean
from numpy import std
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.metrics import precision_recall_fscore_support

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# normalize data in range [0,1]
# a grayscale image has values in [0,255]
x_train = x_train/255
x_test = x_test/255

# plot 9 images as gray scale
for i in range(9):
	plt.subplot(331+i)
	plt.imshow(x_train[i], cmap=plt.get_cmap('gray'))
# show the plot
plt.show()
plt.pause(4)

#do some informative plotting
print('Input data (train) shape is:', x_train.shape)
print('Input data (test) shape is:', x_test.shape)

print('Output data (train) shape is:', y_train.shape)
print('Output data (test) shape is:', y_test.shape)

print('Train set has the following classes:', np.unique(y_train))
print('Test set has the following classes:', np.unique(y_test))

# do a one-hot-encoding for the outputs
y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

#check the outputs' range (just to be sure)
print(np.max(x_train))
print(np.min(x_train))

# define CNN model
def define_model():
	my_cnn = Sequential()
	my_cnn.add(keras.Input(shape=(28,28,1)))
	my_cnn.add(Conv2D(16, kernel_size=(3,3), activation='relu'))
	my_cnn.add(MaxPooling2D(pool_size=(2,2)))
	my_cnn.add(Conv2D(12, kernel_size=(3,3), activation='relu'))
	my_cnn.add(Dropout(0.3))
	my_cnn.add(MaxPooling2D(pool_size=(2,2)))
	my_cnn.add(Flatten())
	my_cnn.add(Dense(128, activation = 'relu'))
	my_cnn.add(Dropout(0.3))
	my_cnn.add(Dense(10, activation = 'softmax'))

	#compile the model
	my_cnn.compile(loss='categorical_crossentropy',\
                optimizer='Adam',\
                metrics=['accuracy'])
	return my_cnn
# define DNN model
def define_DNN_model():
	model = Sequential()
	model.add(keras.Input(shape=(28,28,1)))
	model.add(Flatten())
	model.add(Dense(256, activation='relu'))
	model.add(Dropout(0.3))
	model.add(Dense(128, activation='relu'))
	model.add(Dropout(0.3))
	model.add(Dense(10, activation='softmax'))  # Output layer for 10 classes
	model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
	return model

def evaluate_model(dataX_train, dataY_train, n_folds=6, model_type='CNN'):
  # evaluate the model using k-fold cross-validation
	acc_scores, histories = list(), list()
	results = []
	best_model = None
	best_accuracy = 0

	# prepare cross validation
	kfold = KFold(n_folds, shuffle=True, random_state=1)
	# enumerate splits
	fold_number = 1
	y_test_true = np.argmax(y_test, axis=1)
	for train_ix, test_ix in kfold.split(dataX_train):
		# Select rows for train and test
		train_fold_X, train_fold_Y, val_fold_X, val_fold_Y = dataX_train[train_ix], dataY_train[train_ix], dataX_train[test_ix], dataY_train[test_ix]
		# Define model
		if model_type == 'CNN':
			model = define_model()
		elif model_type == 'DNN':
			model = define_DNN_model()
		# Fit model and record history
		history = model.fit(train_fold_X, train_fold_Y, epochs=20, batch_size=128, validation_data=(val_fold_X, val_fold_Y), verbose=0)
		histories.append(history)
		# Use the fold trained model to calculate the outputs for both fold train and test sets
		y_train_predicted = model.predict(train_fold_X)
		y_test_predicted = model.predict(x_test)
		# Convert CNN outputs to categorical, i.e. 0, 1, 2, ..., numofClasses
		y_train_predicted = np.argmax(y_train_predicted, axis=1)
		y_test_predicted = np.argmax(y_test_predicted, axis=1)
		y_train_true = np.argmax(train_fold_Y, axis=1)
		# Calculate accuracies
		train_acc = np.mean(y_train_predicted == y_train_true)
		test_acc = np.mean(y_test_predicted == y_test_true)
		# Store test set accuracies
		acc_scores.append(test_acc)
		# Determine best model based on test set accuracy
		if test_acc > best_accuracy:
			best_accuracy = test_acc
			best_model = model
		# Calculate the rest of the metrics and store all of them in results
		train_p, train_r, train_f, _ = precision_recall_fscore_support(y_train_true, y_train_predicted, average='macro')
		test_p, test_r, test_f, _ = precision_recall_fscore_support(y_test_true, y_test_predicted, average='macro')
		results.append({ 'Technique': model_type,'Set': 'Train', 'Fold number': fold_number, 'Accuracy': train_acc, 'Precision': round(train_p, 4), 'Recall': round(train_r,4), 'F1 score': round(train_f, 4) })
		results.append({ 'Technique': model_type,'Set': 'Test', 'Fold number': fold_number, 'Accuracy': test_acc, 'Precision': round(test_p, 4), 'Recall': round(test_r,4), 'F1 score': round(test_f, 4) })
		fold_number += 1
	# Create the Dataframe
	results_df = pd.DataFrame(results)
	return results_df, best_model, acc_scores, histories

# plot diagnostic learning curves
def summarize_diagnostics(histories):
	for i in range(len(histories)):
		# plot loss
		plt.subplot(2, 1, 1)
		plt.title('Cross Entropy Loss')
		plt.plot(histories[i].history['loss'], color='blue', label='train')
		plt.plot(histories[i].history['val_loss'], color='orange', label='test')
		plt.xlabel('epoch')
		plt.ylabel(list(histories[i].history.keys())[1])
		plt.legend(['train', 'val'], loc='upper left')

		# plot accuracy
		plt.subplot(2, 1, 2)
		plt.title('Classification Accuracy')
		plt.plot(histories[i].history['accuracy'], color='blue', label='train')
		plt.plot(histories[i].history['val_accuracy'], color='orange', label='test')
		plt.xlabel('epoch')
		plt.ylabel(list(histories[i].history.keys())[0])
		plt.legend(['train', 'val'], loc='upper left')
		plt.tight_layout()  # Add this line to adjust the spacing
	plt.show()

def summarize_performance(scores):
	# print summary
	print('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))
	# box and whisker plots of test accuracy
	plt.boxplot(scores)
	plt.show()

# run the test harness for evaluating a model
def run_test_harness(model_type='CNN'):
	results_df, best_model, scores, histories = evaluate_model(x_train, y_train, model_type= model_type)
	# learning curves
	summarize_diagnostics(histories)
	# summarize accuracy statistics
	summarize_performance(scores)
	# Save model and display summary
	if best_model:
		best_model.save(f'{model_type}.h5')
		best_model.summary()
	return results_df

# Collect results
df1 = run_test_harness("CNN")
df2 = run_test_harness("DNN")
results_df = pd.concat([df1, df2], ignore_index=True)
# Print and save results
print(results_df)
results_df.to_csv('erotima1.csv', index=False)